# Gloss-Free Continuous Sign Language to Text Translation

This repository implements an **end-to-end deep learning pipeline for translating continuous sign language videos directly into natural language text**, without relying on intermediate gloss annotations. The system is designed as part of an academic Masterâ€™s Thesis and focuses on robustness, temporal modelling, and semantic compression.

---

## ğŸ” Overview

Continuous Sign Language Translation (CSLT) is a challenging problem due to long temporal dependencies, co-articulation effects, and lack of explicit word boundaries. This project addresses these challenges by:

- Extracting poseâ€“motion features from video frames
- Modelling long-range temporal dependencies using a Transformer-based bottleneck encoder
- Applying iterative masking and latent smoothing for robustness
- Generating grammatically coherent sentences using a neural text decoder

The complete pipeline consists of **seven clearly defined stages**, from raw video input to final evaluation.

---

## âœ¨ Key Features

- Gloss-free **CSL â†’ Text** translation
- **MediaPipe Holistic** for poseâ€“motion feature extraction
- **Transformer bottleneck encoder** for temporal compression
- **Iterative masking** to improve robustness
- **Conv1D-based smoothing** for latent refinement
- **T5-based text decoder**
- Evaluation using standard machine translation metrics

---

## ğŸ“ Project Structure



csltCode/
â”‚
â”œâ”€â”€ src/ # Core model components
â”‚ â”œâ”€â”€ model.py # Bottleneck Transformer + smoothing
â”‚ â”œâ”€â”€ dataset.py # Dataset loader
â”‚ â””â”€â”€ collate.py # Padding and masking logic
â”‚
â”œâ”€â”€ scripts/ # Executable pipeline stages
â”‚ â”œâ”€â”€ extract_holistic.py # Pose feature extraction
â”‚ â”œâ”€â”€ build_manifest.py # Manifest generation
â”‚ â”œâ”€â”€ train.py # Model training
â”‚ â”œâ”€â”€ eval_all_metrics.py # Evaluation metrics
â”‚ â””â”€â”€ thesis_report.py # Thesis tables and plots
â”‚
â”œâ”€â”€ data/ # (Not included in GitHub)
â”‚ â”œâ”€â”€ raw/ # PHOENIX-2014-T dataset
â”‚ â”œâ”€â”€ features/ # Extracted pose features (.npy)
â”‚ â””â”€â”€ manifests/ # Train / Dev / Test CSV files
â”‚
â”œâ”€â”€ checkpoints/ # Saved model checkpoints
â”œâ”€â”€ results/ # Predictions, metrics, visualisations
â””â”€â”€ README.md


---

## ğŸ“¦ Dataset

This project uses the **PHOENIX-2014-T** dataset for continuous sign language translation.

âš ï¸ **Note:**  
Due to size constraints, the dataset, extracted features, checkpoints, and results are **not included** in this repository.

Expected local dataset structure:


---

## ğŸ“¦ Dataset

This project uses the **PHOENIX-2014-T** dataset for continuous sign language translation.

âš ï¸ **Note:**  
Due to size constraints, the dataset, extracted features, checkpoints, and results are **not included** in this repository.

Expected local dataset structure:
data/raw/PHOENIX-2014-T/


---

## ğŸ”„ Pipeline Stages

### Stage 1 â€” Input Preprocessing
- Organises raw video frames
- Validates alignment between frames and sentence annotations
- Ensures consistent and valid samples

(Provided by the dataset; no script required)

---

### Stage 2 â€” Feature Extraction
- Applies **MediaPipe Holistic** on each frame
- Extracts 225-dimensional poseâ€“motion descriptors
- Produces a temporal feature matrix per sample

Run:
```bash
python scripts/extract_holistic.py \
  --frames_root data/raw/PHOENIX-2014-T/features/fullFrame-210x260px \
  --out_root data/features \
  --split train


Repeat for dev and test.

Output:

data/features/train/*.npy
data/features/dev/*.npy
data/features/test/*.npy


Each feature file has shape:

(
ğ‘‡
,
225
)
(T,225)
Stage 3 â€” Manifest Generation

Links extracted features with text annotations

Produces CSV files used by the training pipeline

Run:

python scripts/build_manifest.py \
  --corpus_csv data/raw/PHOENIX-2014-T/annotations/manual/PHOENIX-2014-T.train.corpus.csv \
  --features_root data/features/train \
  --out data/manifests/train_manifest.csv


Repeat for dev and test splits.

Stage 4â€“6 â€” Model Training

Includes:

Transformer-based bottleneck encoder

Iterative masking of latent tokens

Conv1D-based smoothing

T5 text decoder

Training configuration:

Batch size: 6

Learning rate: 3e-5

Optimizer: AdamW

Mask probability: 0.15

Epochs: 20â€“30+

Run training:

python scripts/train.py


Outputs:

checkpoints/epoch_1.pt
checkpoints/epoch_2.pt
...

Stage 7 â€” Evaluation

Generates translations on the test set

Computes standard translation metrics

Run:

python scripts/eval_all_metrics.py


Output:

results/results_test_predictions.csv

Optional â€” Thesis Reports & Visualisation

Generates tables, plots, and summaries for thesis writing

Run:

python scripts/thesis_report.py


Outputs:

results/plots/
results/tables/
results/summary.txt

â–¶ï¸ End-to-End Execution Order
1. Prepare PHOENIX-2014-T dataset
2. Run extract_holistic.py
3. Run build_manifest.py
4. Run train.py
5. Run eval_all_metrics.py
6. (Optional) Run thesis_report.py

ğŸ“Š Evaluation Metrics

BLEU

chrF

METEOR

Word Error Rate (WER)

Sentence-level Accuracy

These metrics provide complementary insights into lexical overlap, semantic similarity, and word-level alignment.

ğŸ§ª Notes

data/, checkpoints/, and results/ are ignored in GitHub

Only source code and documentation are version-controlled

Designed for academic research and reproducibility

ğŸ“„ License

For academic and research use only.

âœï¸ Author

Shalmon Titre
Masterâ€™s Thesis Project
Gloss-Free Continuous Sign Language Translation
